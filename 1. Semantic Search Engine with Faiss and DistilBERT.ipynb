{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40cf078f",
   "metadata": {},
   "source": [
    "# M1 Semantic Search Engine with Faiss and DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee68704",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Build a search engine using FAISS similarity search library and a pre-trained DistilBERT model from Transformers.\n",
    "\n",
    "\n",
    "- On your search for an optimal document retrieval method in the CDCâ€™s huge knowledge base you decide to implement a semantic search engine to overcome known limitations of statistical (TfIdf) full-text search. Its weaknesses stem from the fact that it relies on counting and matching words in a search query with documents in the database in the document. Even though modern full-text search engines do include, synonyms, for example, still there are many ways to express the same idea. You know that Transformers models excel at contextual learning, so you decide to apply transfer learning with pre-trained BERT models to see if you can make your search engine smarter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21384bd6",
   "metadata": {},
   "source": [
    "## Create a new Jupyter Notebook and load all relevant Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "728afa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f63c33",
   "metadata": {},
   "source": [
    "## Open the provided JSON file called sentences.json. It contains a list of strings (sentences.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c6513fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents\n",
    "with open('data/sentences.json', 'r') as file:\n",
    "    documents = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cbc550",
   "metadata": {},
   "source": [
    "## Use AutoTokenizer and AutoModel classes from Transformers library to load a pre-trained model from Transformers, along with the appropriate tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49614f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57eda341582b42369cd281241520b67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the a BERT model and a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c091026",
   "metadata": {},
   "source": [
    "## Create an empty inverted index with FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55132f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a flat Faiss index\n",
    "index = faiss.IndexIDMap(faiss.IndexFlatIP(768)) # the size of our vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65eee60",
   "metadata": {},
   "source": [
    "## Write an encoder function that inputs a string and outputs a dense PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca5ea839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a function that uses a BERT model to vectorize the texts\n",
    "def encode(document):\n",
    "    # Encode the documents and return vectors\n",
    "    tokens = tokenizer(document, return_tensors='pt')\n",
    "    vector = model(**tokens)[0].detach().squeeze()\n",
    "    return torch.mean(vector, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd2bfdb",
   "metadata": {},
   "source": [
    "## Build a list of modeled vector representations for each document with a reusable encoder function you created in step 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c1f7bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the documents\n",
    "vectors = [encode(d) for d in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94665656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.size() for v in vectors]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f1d40",
   "metadata": {},
   "source": [
    "## Populate the empty FAISS index with the output vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a07a6a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the document vectors into the index. They need to be transformed into numpy arrays first\n",
    "index.add_with_ids(\n",
    "    np.array([v.numpy() for v in vectors]),\n",
    "    # the IDs will be 0 to len(documents)\n",
    "    np.array(range(0, len(documents)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e5f964",
   "metadata": {},
   "source": [
    "## Build a search function that accepts a string query, encodes it, searches similar documents in the index, and returns top 5 results with their top_k scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb87521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a function to search the index and return scored results\n",
    "def search(query, k=5):\n",
    "    # Search the index and return top scored results\n",
    "    encoded_query = encode(query).unsqueeze(dim=0).numpy()\n",
    "    top_k = index.search(encoded_query, k)\n",
    "    scores = top_k[0][0]\n",
    "    results = [documents[_id] for _id in top_k[1][0]]\n",
    "    results = list(zip(results, scores))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a640aec0",
   "metadata": {},
   "source": [
    "## Test your search engine by asking some questions. Check out the attached questions.json for a few suggested questions to start with, but feel free to play around and search for anything you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3492950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The Spanish flu, also known as the 1918 flu pandemic, was an unusually '\n",
      "  'deadly influenza pandemic caused by the H1N1 influenza A virus.',\n",
      "  51.06952),\n",
      " ('As of 2018, approximately 37.9 million people are infected with HIV '\n",
      "  'globally.',\n",
      "  45.203133)]\n"
     ]
    }
   ],
   "source": [
    "pprint(search(\"spanish flu casualties\", k=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9a06e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"How many people have died during Black Death?\", \"Which diseases can be transmitted by animals?\", \"Connection between climate change and a likelihood of a pandemic\", \"What is an example of a latent virus\", \"Viruses in nanotechnology\", \"Giant viruses classification\", \"What are the notable pandemic prevention organizations?\", \"How many leprosy outbreaks are known to happen?\", \"What are the geographic areas with the highest transmission of malaria?\", \"How to prevent the spread of viral infections?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e775d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'How many people have died during Black Death?'\n",
      "[('As of 2018, approximately 37.9 million people are infected with HIV '\n",
      "  'globally.',\n",
      "  52.61343)]\n",
      "'Which diseases can be transmitted by animals?'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  54.049507)]\n",
      "'Connection between climate change and a likelihood of a pandemic'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  60.54062)]\n",
      "'What is an example of a latent virus'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  59.449497)]\n",
      "'Viruses in nanotechnology'\n",
      "[('Current pandemics include COVID-19 (SARS-CoV-2) and HIV/AIDS.', 53.095844)]\n",
      "'Giant viruses classification'\n",
      "[('Current pandemics include COVID-19 (SARS-CoV-2) and HIV/AIDS.', 44.71003)]\n",
      "'What are the notable pandemic prevention organizations?'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  56.514065)]\n",
      "'How many leprosy outbreaks are known to happen?'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  63.531757)]\n",
      "'What are the geographic areas with the highest transmission of malaria?'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  58.686066)]\n",
      "'How to prevent the spread of viral infections?'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  60.5831)]\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    pprint(question)\n",
    "    pprint(search(question, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95db045c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
