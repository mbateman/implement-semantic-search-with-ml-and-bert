{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb4bd92",
   "metadata": {},
   "source": [
    "# M1 Semantic Search Engine with Faiss and DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c252c0",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Build a search engine using FAISS similarity search library and a pre-trained DistilBERT model from Transformers.\n",
    "\n",
    "\n",
    "- On your search for an optimal document retrieval method in the CDCâ€™s huge knowledge base you decide to implement a semantic search engine to overcome known limitations of statistical (TfIdf) full-text search. Its weaknesses stem from the fact that it relies on counting and matching words in a search query with documents in the database in the document. Even though modern full-text search engines do include, synonyms, for example, still there are many ways to express the same idea. You know that Transformers models excel at contextual learning, so you decide to apply transfer learning with pre-trained BERT models to see if you can make your search engine smarter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee45c0",
   "metadata": {},
   "source": [
    "## Create a new Jupyter Notebook and load all relevant Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f121566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e1029a",
   "metadata": {},
   "source": [
    "## Open the provided JSON file called sentences.json. It contains a list of strings (sentences.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d215fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents\n",
    "with open('data/sentences.json', 'r') as file:\n",
    "    documents = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fe1fc1",
   "metadata": {},
   "source": [
    "## Use AutoTokenizer and AutoModel classes from Transformers library to load a pre-trained model from Transformers, along with the appropriate tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72c68af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57eda341582b42369cd281241520b67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the a BERT model and a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd8b81",
   "metadata": {},
   "source": [
    "## Create an empty inverted index with FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de76508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a flat Faiss index\n",
    "index = faiss.IndexIDMap(faiss.IndexFlatIP(768)) # the size of our vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1283be52",
   "metadata": {},
   "source": [
    "## Write an encoder function that inputs a string and outputs a dense PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf72c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a function that uses a BERT model to vectorize the texts\n",
    "def encode(document):\n",
    "    # Encode the documents and return vectors\n",
    "    tokens = tokenizer(document, return_tensors='pt')\n",
    "    vector = model(**tokens)[0].detach().squeeze()\n",
    "    return torch.mean(vector, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fa2265",
   "metadata": {},
   "source": [
    "## Build a list of modeled vector representations for each document with a reusable encoder function you created in step 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0d48c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the documents\n",
    "vectors = [encode(d) for d in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b082b0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.size() for v in vectors]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9747cb",
   "metadata": {},
   "source": [
    "## Populate the empty FAISS index with the output vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f0b1abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the document vectors into the index. They need to be transformed into numpy arrays first\n",
    "index.add_with_ids(\n",
    "    np.array([v.numpy() for v in vectors]),\n",
    "    # the IDs will be 0 to len(documents)\n",
    "    np.array(range(0, len(documents)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a5d54",
   "metadata": {},
   "source": [
    "## Build a search function that accepts a string query, encodes it, searches similar documents in the index, and returns top 5 results with their top_k scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b097beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a function to search the index and return scored results\n",
    "def search(query, k=5):\n",
    "    # Search the index and return top scored results\n",
    "    encoded_query = encode(query).unsqueeze(dim=0).numpy()\n",
    "    top_k = index.search(encoded_query, k)\n",
    "    scores = top_k[0][0]\n",
    "    results = [documents[_id] for _id in top_k[1][0]]\n",
    "    results = list(zip(results, scores))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16925728",
   "metadata": {},
   "source": [
    "## Test your search engine by asking some questions. Check out the attached questions.json for a few suggested questions to start with, but feel free to play around and search for anything you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "983f22b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The Spanish flu, also known as the 1918 flu pandemic, was an unusually '\n",
      "  'deadly influenza pandemic caused by the H1N1 influenza A virus.',\n",
      "  51.06952),\n",
      " ('As of 2018, approximately 37.9 million people are infected with HIV '\n",
      "  'globally.',\n",
      "  45.203133)]\n"
     ]
    }
   ],
   "source": [
    "pprint(search(\"spanish flu casualties\", k=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7433887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"How many people have died during Black Death?\", \"Which diseases can be transmitted by animals?\", \"Connection between climate change and a likelihood of a pandemic\", \"What is an example of a latent virus\", \"Viruses in nanotechnology\", \"Giant viruses classification\", \"What are the notable pandemic prevention organizations?\", \"How many leprosy outbreaks are known to happen?\", \"What are the geographic areas with the highest transmission of malaria?\", \"How to prevent the spread of viral infections?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cd347b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'How many people have died during Black Death?'\n",
      "[('As of 2018, approximately 37.9 million people are infected with HIV '\n",
      "  'globally.',\n",
      "  52.61343)]\n",
      "'Which diseases can be transmitted by animals?'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  54.049507)]\n",
      "'Connection between climate change and a likelihood of a pandemic'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  60.54062)]\n",
      "'What is an example of a latent virus'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  59.449497)]\n",
      "'Viruses in nanotechnology'\n",
      "[('Current pandemics include COVID-19 (SARS-CoV-2) and HIV/AIDS.', 53.095844)]\n",
      "'Giant viruses classification'\n",
      "[('Current pandemics include COVID-19 (SARS-CoV-2) and HIV/AIDS.', 44.71003)]\n",
      "'What are the notable pandemic prevention organizations?'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  56.514065)]\n",
      "'How many leprosy outbreaks are known to happen?'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  63.531757)]\n",
      "'What are the geographic areas with the highest transmission of malaria?'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  58.686066)]\n",
      "'How to prevent the spread of viral infections?'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  60.5831)]\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    pprint(question)\n",
    "    pprint(search(question, k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d9a449",
   "metadata": {},
   "source": [
    "# M2 Searching Long Documents with Sentence Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b413b",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Implement a search engine using sentence-transformers and FAISS.\n",
    "\n",
    "\n",
    "- In this milestone, instead of a base BERT model like we did previously, we will use Sentence BERT (SBERT). SBERT was developed to tackle similarity search and unsupervised clustering problems, for which classic BERT is not a good candidate.\n",
    "\n",
    "\n",
    "- SBERT outputs fix embeddings for entire sentences and paragraphs instead of tokens and allow to drastically reduce computation time while keeping exceptional accuracy.\n",
    "\n",
    "\n",
    "- You have successfully implemented your first prototype of a semantic similarity search engine using FAISS, a library for similarity search, and DistilBERT in Milestone 1 of this project. However, you are asking yourself what to do about the fact that most documents in the CDCâ€™s database are much longer than the limit that this model can work with. Luckily, youâ€™ve found out that there is another library called sentence-transformers that just might work for you! The models in this library have been developed and trained by the leading NLP researchers to extract meaningful embeddings for longer texts. Letâ€™s test it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7289945",
   "metadata": {},
   "source": [
    "## Import the following libraries into a Jupyter or Colab Notebook:\n",
    "\n",
    "- JSON\n",
    "- FAISS\n",
    "- sentence-transformers\n",
    "- PyTorch\n",
    "- NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2602b42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\r\n"
     ]
    }
   ],
   "source": [
    "!cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "170bf32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d497749",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83014ec",
   "metadata": {},
   "source": [
    "## Load and open the provided data.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67e68859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents\n",
    "with open('data/data.json', 'r') as file:\n",
    "    documents = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cbf715e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A pandemic (from Greek Ï€á¾¶Î½, pan, \"all\" and Î´á¿†Î¼Î¿Ï‚, demos, \"people\") is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people. A widespread endemic disease with a stable number of infected people is not a pandemic. Widespread endemic diseases with a stable number of infected people such as recurrences of seasonal influenza are generally excluded as they occur simultaneously in large regions of the globe rather than being spread worldwide.\\nThroughout human history, there have been a number of pandemics of diseases such as smallpox and tuberculosis. The most fatal pandemic in recorded history was the Black Death (also known as The Plague), which killed an estimated 75â€“200 million people in the 14th century. The term was not used yet but was for later pandemics including the 1918 influenza pandemic (Spanish flu). Current pandemics include COVID-19 (SARS-CoV-2) and HIV/AIDS.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdd714a",
   "metadata": {},
   "source": [
    "## Compute the sentence embeddings for documents in the data.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68fd4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [d['text'] for d in documents]\n",
    "corpus_embeddings = embedder.encode(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c53b2440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91f964a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(corpus_embeddings).shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439453c7",
   "metadata": {},
   "source": [
    "## Create an empty FAISS index and with the new documents exactly the same way as you did in M1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9694d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33d220c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexIDMap(faiss.IndexFlatIP(768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e7606a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors = [encode(d) for d in corpus]\n",
    "# RuntimeError: The size of tensor a (668) must match the size of tensor b (512) at non-singleton dimension 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbefb5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add_with_ids(\n",
    "    embeddings,\n",
    "    # the IDs will be 0 to len(documents)\n",
    "    np.array(range(0, len(documents)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d2c33b",
   "metadata": {},
   "source": [
    "## Just like in Milestone 1, build a search function to retrieve the top 5 most similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9781d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(np.array([embeddings[20]]), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a5cb81dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[184.574   , 100.64257 ,  99.096565,  83.06283 ,  77.55721 ,\n",
       "          75.874825,  75.54649 ,  75.020515,  74.691605,  74.33931 ]],\n",
       "       dtype=float32),\n",
       " array([[20, 22, 10, 17, 21, 19, 25, 15,  0,  6]]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc15d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a search function that finds the most relevant search results\n",
    "def search(query, corpus=corpus, k=5):\n",
    "    # Compute query embeddings\n",
    "    vector = embedder.encode(list(query))\n",
    "    # Search top results in the Faiss index\n",
    "    D, I = index.search(np.array(vector).astype(\"float32\"), k=k)\n",
    "    results = [corpus[i] for i in I[0]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c41d2",
   "metadata": {},
   "source": [
    "## Test the search function with a few queries. Use the provided questions.json for a few ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c636ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how many people died from black death?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ad069a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top search results:\n",
      "Swine influenza is an infection caused by any one of several types of swine influenza viruses. Swine influenza virus (SIV) or swine-origin influenza virus (S-OIV) is any strain of the influenza family of viruses that is endemic in pigs. As of 2009, the known SIV strains include influenza C and the subtypes of  influenza A known as H1N1, H1N2, H2N1, H3N1, H3N2, and H2N3.\n",
      "Swine influenza virus is common throughout pig populations worldwide. Transmission of the virus from pigs to humans is not common and does not always lead to human flu, often resulting only in the production of antibodies in the blood. If transmission does cause human flu, it is called zoonotic swine flu. People with regular exposure to pigs are at increased risk of swine flu infection.\n",
      "Around the mid-20th century, identification of influenza subtypes became possible, allowing accurate diagnosis of transmission to humans. Since then, only 50 such transmissions have been confirmed. These strains of swine flu rarely pass from human to human. Symptoms of zoonotic swine flu in humans are similar to those of influenza and of influenza-like illness in general, namely chills, fever, sore throat, muscle pains, severe headache, coughing, weakness, shortness of breath, and general discomfort.\n",
      "It is estimated that in the 2009 flu pandemic 11â€“21% of the then global population (of about 6.8 billion), or around 700 million to 1.4 billion people, contracted the illnessâ€”more in absolute terms than the Spanish flu pandemic. Actual fatalities ranged between 12,000 and 18,000. However, in a 2012 study, the CDC estimated more than 284,000 possible fatalities worldwide, with range from 150,000 to 575,000.\n",
      "In August 2010, the World Health Organization declared the swine flu pandemic officially over.Subsequent cases of swine flu were reported in India in 2015, with over 31,156 positive test cases and 1,841 deaths up to March 2015.\n",
      "The Pandemic Severity Assessment Framework (PSAF) is an evaluation framework which uses quadrants to evaluate both the transmissibility and clinical severity of a pandemic and to combine these into an overall impact estimate.\n",
      "Clinical severity is calculated via multiple measures including case fatality rate, case-hospitalization ratios, and deaths-hospitalizations ratios, while viral transmissibility is measured via available data among secondary household attack rates, school attack rates, workplace attack rates, community attack rates, rates of emergency department and outpatient visits for influenza-like illness.The PSAF superseded the 2007 linear Pandemic Severity Index (PSI), which assumed 30% spread and measured case fatality rate (CFR) to assess the severity and evolution of the pandemic. The United States Centers for Disease Control and Prevention (CDC) adopted the PSAF as its official pandemic severity assessment tool in 2014, and it was the official pandemic severity assessment tool listed in the CDC's National Pandemic Strategy at the time of the COVID-19 pandemic.\n",
      "PREDICT was an epidemiological research program funded by a United States Agency for International Development (USAID) grant.  Launched in 2009, the program was described as an early warning pandemic system.\n",
      "Viral load, also known as viral burden, viral titre or viral titer, is a numerical expression of the quantity of virus in a given volume of fluid; sputum and blood plasma being two bodily fluids. For example, the viral load of norovirus can be determined from run-off water on garden produce. Norovirus has not only prolonged viral shedding and has the ability to survive in the environment but a minuscule infectious dose is required to produce infection in humans: less than 100 viral particles.Viral load is often expressed as viral particles, or infectious particles per mL depending on the type of assay. A higher viral burden, titre, or viral load often correlates with the severity of an active viral infection. The quantity of virus / mL can be calculated by estimating the live amount of virus in an involved fluid. For example, it can be given in RNA copies per millilitre of blood plasma.\n",
      "Tracking viral load is used to monitor therapy during chronic viral infections, and in immunocompromised patients such as those recovering from bone marrow or solid organ transplantation. Currently, routine testing is available for HIV-1, cytomegalovirus, hepatitis B virus, and hepatitis C virus. Viral load monitoring for HIV is of particular interest in the treatment of people with HIV, as this is continually discussed in the context of management of HIV/AIDS.\n",
      "The COVID-19 pandemic, also known as the coronavirus pandemic, is an ongoing pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The disease was first identified in December 2019 in Wuhan, China. The outbreak was declared a Public Health Emergency of International Concern in January 2020, and a pandemic in March 2020. As of 17 October 2020, more than 39.5 million cases have been confirmed, with more than 1.1 million deaths attributed to COVID-19.\n",
      "\n",
      "Common symptoms include fever, cough, fatigue, breathing difficulties, and loss of smell. Complications may include pneumonia and acute respiratory distress syndrome. The incubation period is typically around five days but may range from one to 14 days. There are several vaccine candidates in development, although none have proven their safety and efficacy. There is no known specific antiviral medication, so primary treatment is currently symptomatic.\n",
      "Recommended preventive measures include hand washing, covering mouth or wearing face mask when sneezing or coughing, social distancing, disinfecting surfaces, ventilation and air-filtering, and monitoring and self-isolation if exposed or symptomatic. Travel restrictions, lockdowns, workplace hazard controls, and facility closures have been implemented. Many places have also worked to increase testing capacity and trace contacts of the infected. These have caused social and economic disruption, including the largest global recession since the Great Depression. Extreme poverty and global famines are affecting hundreds of millions, inflamed by supply shortages. Many events, the environment and education systems have also been affected. Misinformation about the virus has circulated globally. There have been many incidents of xenophobia and racism against Chinese people and against those perceived as being Chinese or as being from areas with high infection rates.\n"
     ]
    }
   ],
   "source": [
    "# Print out the results\n",
    "query = \"How many people died during Black plague?\"\n",
    "results=search(query)\n",
    "\n",
    "print('Top search results:')\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc7ee2c",
   "metadata": {},
   "source": [
    "## Write the FAISS index to file. It will be useful for the next, and final, milestone!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bb3791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"data/index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30b3572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
