{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ab13df",
   "metadata": {},
   "source": [
    "# M1 Semantic Search Engine with Faiss and DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325c67c7",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Build a search engine using FAISS similarity search library and a pre-trained DistilBERT model from Transformers.\n",
    "\n",
    "\n",
    "- On your search for an optimal document retrieval method in the CDC’s huge knowledge base you decide to implement a semantic search engine to overcome known limitations of statistical (TfIdf) full-text search. Its weaknesses stem from the fact that it relies on counting and matching words in a search query with documents in the database in the document. Even though modern full-text search engines do include, synonyms, for example, still there are many ways to express the same idea. You know that Transformers models excel at contextual learning, so you decide to apply transfer learning with pre-trained BERT models to see if you can make your search engine smarter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c1a749",
   "metadata": {},
   "source": [
    "## Create a new Jupyter Notebook and load all relevant Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5858126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b93e2b",
   "metadata": {},
   "source": [
    "## Open the provided JSON file called sentences.json. It contains a list of strings (sentences.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8ecc629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents\n",
    "with open('data/sentences.json', 'r') as file:\n",
    "    documents = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4c9d70",
   "metadata": {},
   "source": [
    "## Use AutoTokenizer and AutoModel classes from Transformers library to load a pre-trained model from Transformers, along with the appropriate tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba063d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57eda341582b42369cd281241520b67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the a BERT model and a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecc6174",
   "metadata": {},
   "source": [
    "## Create an empty inverted index with FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80c489bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a flat Faiss index\n",
    "index = faiss.IndexIDMap(faiss.IndexFlatIP(768)) # the size of our vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85370997",
   "metadata": {},
   "source": [
    "## Write an encoder function that inputs a string and outputs a dense PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "476a6c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a function that uses a BERT model to vectorize the texts\n",
    "def encode(document):\n",
    "    # Encode the documents and return vectors\n",
    "    tokens = tokenizer(document, return_tensors='pt')\n",
    "    vector = model(**tokens)[0].detach().squeeze()\n",
    "    return torch.mean(vector, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2a8a2",
   "metadata": {},
   "source": [
    "## Build a list of modeled vector representations for each document with a reusable encoder function you created in step 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d33d5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the documents\n",
    "vectors = [encode(d) for d in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b95f179f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.size() for v in vectors]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb87fd1d",
   "metadata": {},
   "source": [
    "## Populate the empty FAISS index with the output vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8922618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the document vectors into the index. They need to be transformed into numpy arrays first\n",
    "index.add_with_ids(\n",
    "    np.array([v.numpy() for v in vectors]),\n",
    "    # the IDs will be 0 to len(documents)\n",
    "    np.array(range(0, len(documents)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a68d00",
   "metadata": {},
   "source": [
    "## Build a search function that accepts a string query, encodes it, searches similar documents in the index, and returns top 5 results with their top_k scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75d336f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a function to search the index and return scored results\n",
    "def search(query, k=5):\n",
    "    # Search the index and return top scored results\n",
    "    encoded_query = encode(query).unsqueeze(dim=0).numpy()\n",
    "    top_k = index.search(encoded_query, k)\n",
    "    scores = top_k[0][0]\n",
    "    results = [documents[_id] for _id in top_k[1][0]]\n",
    "    results = list(zip(results, scores))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b3f4bd",
   "metadata": {},
   "source": [
    "## Test your search engine by asking some questions. Check out the attached questions.json for a few suggested questions to start with, but feel free to play around and search for anything you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e795f9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The Spanish flu, also known as the 1918 flu pandemic, was an unusually '\n",
      "  'deadly influenza pandemic caused by the H1N1 influenza A virus.',\n",
      "  51.06952),\n",
      " ('As of 2018, approximately 37.9 million people are infected with HIV '\n",
      "  'globally.',\n",
      "  45.203133)]\n"
     ]
    }
   ],
   "source": [
    "pprint(search(\"spanish flu casualties\", k=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ed41dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"How many people have died during Black Death?\", \"Which diseases can be transmitted by animals?\", \"Connection between climate change and a likelihood of a pandemic\", \"What is an example of a latent virus\", \"Viruses in nanotechnology\", \"Giant viruses classification\", \"What are the notable pandemic prevention organizations?\", \"How many leprosy outbreaks are known to happen?\", \"What are the geographic areas with the highest transmission of malaria?\", \"How to prevent the spread of viral infections?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "199e2eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'How many people have died during Black Death?'\n",
      "[('As of 2018, approximately 37.9 million people are infected with HIV '\n",
      "  'globally.',\n",
      "  52.61343)]\n",
      "'Which diseases can be transmitted by animals?'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  54.049507)]\n",
      "'Connection between climate change and a likelihood of a pandemic'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  60.54062)]\n",
      "'What is an example of a latent virus'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  59.449497)]\n",
      "'Viruses in nanotechnology'\n",
      "[('Current pandemics include COVID-19 (SARS-CoV-2) and HIV/AIDS.', 53.095844)]\n",
      "'Giant viruses classification'\n",
      "[('Current pandemics include COVID-19 (SARS-CoV-2) and HIV/AIDS.', 44.71003)]\n",
      "'What are the notable pandemic prevention organizations?'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  56.514065)]\n",
      "'How many leprosy outbreaks are known to happen?'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  63.531757)]\n",
      "'What are the geographic areas with the highest transmission of malaria?'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  58.686066)]\n",
      "'How to prevent the spread of viral infections?'\n",
      "[('A pandemic is an epidemic of an infectious disease that has spread across a '\n",
      "  'large region, for instance multiple continents or worldwide, affecting a '\n",
      "  'substantial number of people.',\n",
      "  60.5831)]\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    pprint(question)\n",
    "    pprint(search(question, k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29c032",
   "metadata": {},
   "source": [
    "# M2 Searching Long Documents with Sentence Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ee5ac",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Implement a search engine using sentence-transformers and FAISS.\n",
    "\n",
    "\n",
    "- In this milestone, instead of a base BERT model like we did previously, we will use Sentence BERT (SBERT). SBERT was developed to tackle similarity search and unsupervised clustering problems, for which classic BERT is not a good candidate.\n",
    "\n",
    "\n",
    "- SBERT outputs fix embeddings for entire sentences and paragraphs instead of tokens and allow to drastically reduce computation time while keeping exceptional accuracy.\n",
    "\n",
    "\n",
    "- You have successfully implemented your first prototype of a semantic similarity search engine using FAISS, a library for similarity search, and DistilBERT in Milestone 1 of this project. However, you are asking yourself what to do about the fact that most documents in the CDC’s database are much longer than the limit that this model can work with. Luckily, you’ve found out that there is another library called sentence-transformers that just might work for you! The models in this library have been developed and trained by the leading NLP researchers to extract meaningful embeddings for longer texts. Let’s test it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafd306a",
   "metadata": {},
   "source": [
    "## Import the following libraries into a Jupyter or Colab Notebook:\n",
    "\n",
    "- JSON\n",
    "- FAISS\n",
    "- sentence-transformers\n",
    "- PyTorch\n",
    "- NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dd408cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\r\n"
     ]
    }
   ],
   "source": [
    "!cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9af732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0138d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14172d",
   "metadata": {},
   "source": [
    "## Load and open the provided data.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c61bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents\n",
    "with open('data/data.json', 'r') as file:\n",
    "    documents = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e691ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A pandemic (from Greek πᾶν, pan, \"all\" and δῆμος, demos, \"people\") is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people. A widespread endemic disease with a stable number of infected people is not a pandemic. Widespread endemic diseases with a stable number of infected people such as recurrences of seasonal influenza are generally excluded as they occur simultaneously in large regions of the globe rather than being spread worldwide.\\nThroughout human history, there have been a number of pandemics of diseases such as smallpox and tuberculosis. The most fatal pandemic in recorded history was the Black Death (also known as The Plague), which killed an estimated 75–200 million people in the 14th century. The term was not used yet but was for later pandemics including the 1918 influenza pandemic (Spanish flu). Current pandemics include COVID-19 (SARS-CoV-2) and HIV/AIDS.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359be5a",
   "metadata": {},
   "source": [
    "## Compute the sentence embeddings for documents in the data.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f56d00f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [d['text'] for d in documents]\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "583ba50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(corpus_embeddings).shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0ad7b",
   "metadata": {},
   "source": [
    "## Create an empty FAISS index and with the new documents exactly the same way as you did in M1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93656009",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "919ab40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexIDMap(faiss.IndexFlatIP(768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea07d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors = [encode(d) for d in corpus]\n",
    "# RuntimeError: The size of tensor a (668) must match the size of tensor b (512) at non-singleton dimension 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5bdd960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add_with_ids(\n",
    "    embeddings,\n",
    "    # the IDs will be 0 to len(documents)\n",
    "    np.array(range(0, len(corpus)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3095ed",
   "metadata": {},
   "source": [
    "## Just like in Milestone 1, build a search function to retrieve the top 5 most similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ee9ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(np.array([embeddings[20]]), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a8846733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[184.57399 , 100.6426  ,  99.09663 ,  83.06282 ,  77.5573  ,\n",
       "          75.87485 ,  75.54655 ,  75.020515,  74.69157 ,  74.33935 ]],\n",
       "       dtype=float32),\n",
       " array([[20, 22, 10, 17, 21, 19, 25, 15,  0,  6]]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4f807e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a search function that finds the most relevant search results\n",
    "def search(query, corpus=corpus, k=5):\n",
    "    # Compute query embeddings\n",
    "    vector = embedder.encode(list(query))\n",
    "    # Search top results in the Faiss index\n",
    "    D, I = index.search(np.array(vector).astype(\"float32\"), k=k)\n",
    "    scores = D[0]\n",
    "    results = [corpus[i] for i in I[0]]\n",
    "    return list(zip(results, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524801d",
   "metadata": {},
   "source": [
    "## Test the search function with a few queries. Use the provided questions.json for a few ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c53bac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how many people died from black death?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5df4816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Swine influenza is an infection caused by any one of several types of swine '\n",
      "  'influenza viruses. Swine influenza virus (SIV) or swine-origin influenza '\n",
      "  'virus (S-OIV) is any strain of the influenza family of viruses that is '\n",
      "  'endemic in pigs. As of 2009, the known SIV strains include influenza C and '\n",
      "  'the subtypes of  influenza A known as H1N1, H1N2, H2N1, H3N1, H3N2, and '\n",
      "  'H2N3.\\n'\n",
      "  'Swine influenza virus is common throughout pig populations worldwide. '\n",
      "  'Transmission of the virus from pigs to humans is not common and does not '\n",
      "  'always lead to human flu, often resulting only in the production of '\n",
      "  'antibodies in the blood. If transmission does cause human flu, it is called '\n",
      "  'zoonotic swine flu. People with regular exposure to pigs are at increased '\n",
      "  'risk of swine flu infection.\\n'\n",
      "  'Around the mid-20th century, identification of influenza subtypes became '\n",
      "  'possible, allowing accurate diagnosis of transmission to humans. Since '\n",
      "  'then, only 50 such transmissions have been confirmed. These strains of '\n",
      "  'swine flu rarely pass from human to human. Symptoms of zoonotic swine flu '\n",
      "  'in humans are similar to those of influenza and of influenza-like illness '\n",
      "  'in general, namely chills, fever, sore throat, muscle pains, severe '\n",
      "  'headache, coughing, weakness, shortness of breath, and general discomfort.\\n'\n",
      "  'It is estimated that in the 2009 flu pandemic 11–21% of the then global '\n",
      "  'population (of about 6.8 billion), or around 700 million to 1.4 billion '\n",
      "  'people, contracted the illness—more in absolute terms than the Spanish flu '\n",
      "  'pandemic. Actual fatalities ranged between 12,000 and 18,000. However, in a '\n",
      "  '2012 study, the CDC estimated more than 284,000 possible fatalities '\n",
      "  'worldwide, with range from 150,000 to 575,000.\\n'\n",
      "  'In August 2010, the World Health Organization declared the swine flu '\n",
      "  'pandemic officially over.Subsequent cases of swine flu were reported in '\n",
      "  'India in 2015, with over 31,156 positive test cases and 1,841 deaths up to '\n",
      "  'March 2015.',\n",
      "  27.466232),\n",
      " ('The Pandemic Severity Assessment Framework (PSAF) is an evaluation '\n",
      "  'framework which uses quadrants to evaluate both the transmissibility and '\n",
      "  'clinical severity of a pandemic and to combine these into an overall impact '\n",
      "  'estimate.\\n'\n",
      "  'Clinical severity is calculated via multiple measures including case '\n",
      "  'fatality rate, case-hospitalization ratios, and deaths-hospitalizations '\n",
      "  'ratios, while viral transmissibility is measured via available data among '\n",
      "  'secondary household attack rates, school attack rates, workplace attack '\n",
      "  'rates, community attack rates, rates of emergency department and outpatient '\n",
      "  'visits for influenza-like illness.The PSAF superseded the 2007 linear '\n",
      "  'Pandemic Severity Index (PSI), which assumed 30% spread and measured case '\n",
      "  'fatality rate (CFR) to assess the severity and evolution of the pandemic. '\n",
      "  'The United States Centers for Disease Control and Prevention (CDC) adopted '\n",
      "  'the PSAF as its official pandemic severity assessment tool in 2014, and it '\n",
      "  \"was the official pandemic severity assessment tool listed in the CDC's \"\n",
      "  'National Pandemic Strategy at the time of the COVID-19 pandemic.',\n",
      "  23.66879)]\n"
     ]
    }
   ],
   "source": [
    "# Print out the results\n",
    "query = \"How many people died during Black plague?\"\n",
    "pprint(search(query, corpus, k=2))\n",
    "# results=search(query)\n",
    "\n",
    "# print('Top search results:')\n",
    "# for result in results:\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09da711d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The Pandemic Severity Assessment Framework (PSAF) is an evaluation '\n",
      "  'framework which uses quadrants to evaluate both the transmissibility and '\n",
      "  'clinical severity of a pandemic and to combine these into an overall impact '\n",
      "  'estimate.\\n'\n",
      "  'Clinical severity is calculated via multiple measures including case '\n",
      "  'fatality rate, case-hospitalization ratios, and deaths-hospitalizations '\n",
      "  'ratios, while viral transmissibility is measured via available data among '\n",
      "  'secondary household attack rates, school attack rates, workplace attack '\n",
      "  'rates, community attack rates, rates of emergency department and outpatient '\n",
      "  'visits for influenza-like illness.The PSAF superseded the 2007 linear '\n",
      "  'Pandemic Severity Index (PSI), which assumed 30% spread and measured case '\n",
      "  'fatality rate (CFR) to assess the severity and evolution of the pandemic. '\n",
      "  'The United States Centers for Disease Control and Prevention (CDC) adopted '\n",
      "  'the PSAF as its official pandemic severity assessment tool in 2014, and it '\n",
      "  \"was the official pandemic severity assessment tool listed in the CDC's \"\n",
      "  'National Pandemic Strategy at the time of the COVID-19 pandemic.',\n",
      "  22.716043),\n",
      " ('Viral load, also known as viral burden, viral titre or viral titer, is a '\n",
      "  'numerical expression of the quantity of virus in a given volume of fluid; '\n",
      "  'sputum and blood plasma being two bodily fluids. For example, the viral '\n",
      "  'load of norovirus can be determined from run-off water on garden produce. '\n",
      "  'Norovirus has not only prolonged viral shedding and has the ability to '\n",
      "  'survive in the environment but a minuscule infectious dose is required to '\n",
      "  'produce infection in humans: less than 100 viral particles.Viral load is '\n",
      "  'often expressed as viral particles, or infectious particles per mL '\n",
      "  'depending on the type of assay. A higher viral burden, titre, or viral load '\n",
      "  'often correlates with the severity of an active viral infection. The '\n",
      "  'quantity of virus / mL can be calculated by estimating the live amount of '\n",
      "  'virus in an involved fluid. For example, it can be given in RNA copies per '\n",
      "  'millilitre of blood plasma.\\n'\n",
      "  'Tracking viral load is used to monitor therapy during chronic viral '\n",
      "  'infections, and in immunocompromised patients such as those recovering from '\n",
      "  'bone marrow or solid organ transplantation. Currently, routine testing is '\n",
      "  'available for HIV-1, cytomegalovirus, hepatitis B virus, and hepatitis C '\n",
      "  'virus. Viral load monitoring for HIV is of particular interest in the '\n",
      "  'treatment of people with HIV, as this is continually discussed in the '\n",
      "  'context of management of HIV/AIDS.',\n",
      "  21.104145)]\n"
     ]
    }
   ],
   "source": [
    "pprint(search(\"spanish flu casualties\",corpus, k=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06a1729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = search(\"spanish flu casualties\",corpus, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7d91c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Pandemic Severity Assessment Framework (PSAF) is an evaluation framework which uses quadrants to evaluate both the transmissibility and clinical severity of a pandemic and to combine these into an overall impact estimate.\\nClinical severity is calculated via multiple measures including case fatality rate, case-hospitalization ratios, and deaths-hospitalizations ratios, while viral transmissibility is measured via available data among secondary household attack rates, school attack rates, workplace attack rates, community attack rates, rates of emergency department and outpatient visits for influenza-like illness.The PSAF superseded the 2007 linear Pandemic Severity Index (PSI), which assumed 30% spread and measured case fatality rate (CFR) to assess the severity and evolution of the pandemic. The United States Centers for Disease Control and Prevention (CDC) adopted the PSAF as its official pandemic severity assessment tool in 2014, and it was the official pandemic severity assessment tool listed in the CDC's National Pandemic Strategy at the time of the COVID-19 pandemic.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79af48b",
   "metadata": {},
   "source": [
    "## Write the FAISS index to file. It will be useful for the next, and final, milestone!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0b43a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"data/pandemics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc1d867",
   "metadata": {},
   "source": [
    "# M3 Question Answering with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb2c54e",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Create a question answering agent powered by sentence-transformers, FAISS similarity search engine, and a BERT model for question answering.\n",
    "\n",
    "\n",
    "- In this milestone, we will try another BERT model (BERT for question answering) to see how we can not only retrieve a relevant text to the query but also derive the exact answer to our question from the passage. BERT models for question answering compute the probability of each token in a text to be the first or the last token in the answer to the question and return all tokens that are situated between the two, inclusively. This is called extractive question answering.\n",
    "\n",
    "\n",
    "- As before, we need to be careful when choosing the model: It has to be finetuned for the specific task we are trying to perform, and we need to be mindful of the computational resources available to us when selecting it. The latter consideration is important to make sure that the model can complete this task in a reasonable time.\n",
    "\n",
    "\n",
    "- You are pleased with your semantic search approach. Sentence-transformers return useful results and your CDC document search project is going full steam. This is really helping your colleagues to speed up their research and develop effective pandemic prevention procedures. Now your colleges have faith that you can help them be even more efficient by returning the relevant information in a more concise form. They hope you can make a question answering agent so that they don’t have to read the entire article to get an answer. You realized that Transformers library provides models trained and fine-tuned for question answering too. You decide to try it, and voila! Just like that, you’ve just built your very own mini-Google!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1854b2",
   "metadata": {},
   "source": [
    "## Create a new notebook and import the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5d710604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee3e00",
   "metadata": {},
   "source": [
    "## Load the FAISS document index you created and saved in Milestone 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "553d0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Faiss index cretaed in Milestone 2\n",
    "index = faiss.read_index('./data/pandemics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe651b60",
   "metadata": {},
   "source": [
    "## Setup a transformers pipeline for question answering. \n",
    "\n",
    "We are going to use one of Transformers models, fine-tuned on SQUAD-v2 dataset (stands for Stanford Question Answering Dataset 2.0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a4bf6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a transformers model\n",
    "embedder = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ktrapeznikov/albert-xlarge-v2-squad-v2\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"ktrapeznikov/albert-xlarge-v2-squad-v2\")\n",
    "# Load a question answering pipeline from Hugging Face Transformers\n",
    "nlp = pipeline('question-answering', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "142045b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.5265803337097168,\n",
       " 'start': 33,\n",
       " 'end': 96,\n",
       " 'answer': ' the task of extracting an answer from a text given a question.'}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n",
    "\"\"\"\n",
    "result = nlp(question=\"What is extractive question answering?\", context=context)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997f9e0d",
   "metadata": {},
   "source": [
    "## Rewrite the search function from Milestone 2 with an added functionality. \n",
    "\n",
    "It should encode a query, search within your FAISS library and extract answers from text using the question-answering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6e9256fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the search function from Milestone 2\n",
    "# to add question-answering functionality\n",
    "def find_answer(query: str, k=5):\n",
    "    # encode the query~\n",
    "    encoded_embeddings = embedder.encode([query])\n",
    "    # Search results in the Faiss index\n",
    "    _, results = index.search(encoded_embeddings, k)\n",
    "    # Use the transformers question-answering pipeline to find answer in text\n",
    "    contexts = [corpus[i] for i in results[0]]\n",
    "    answers = [nlp(question=query, context=context) for context in contexts]\n",
    "    return sorted(answers, key=lambda x: x['score'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "13ef0dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'answer': ' higher-than-expected mortality rate for young adults.',\n",
      "  'end': 1373,\n",
      "  'score': 0.07739856094121933,\n",
      "  'start': 1319},\n",
      " {'answer': ' Actual fatalities ranged between 12,000 and 18,000.',\n",
      "  'end': 1549,\n",
      "  'score': 0.052253905683755875,\n",
      "  'start': 1497}]\n"
     ]
    }
   ],
   "source": [
    "#Checking for the query string which was used in Milestone1 and Milestone2 for comparison\n",
    "pprint(find_answer(\"spanish flu casualties\", k=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95ee947",
   "metadata": {},
   "source": [
    "## Choose a query and search the FAISS index. \n",
    "\n",
    "Pass the search results along with your query to the question answering pipeline, and see how well it can extract and rank the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0ea72db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top search results:\n",
      "{'score': 0.2111435830593109, 'start': 2751, 'end': 2762, 'answer': ' condom use'}\n",
      "{'score': 0.06335747987031937, 'start': 899, 'end': 946, 'answer': '\\nTracking viral load is used to monitor therapy'}\n",
      "{'score': 0.05168566107749939, 'start': 217, 'end': 244, 'answer': ' administration of vaccines'}\n",
      "{'score': 0.041904352605342865, 'start': 110, 'end': 119, 'answer': ' measures'}\n",
      "{'score': 0.0002497527457308024, 'start': 1257, 'end': 1274, 'answer': ' Cholera vaccines'}\n"
     ]
    }
   ],
   "source": [
    "query = \"How to prevent the spread of viral infections?\"\n",
    "results=find_answer(query)\n",
    "\n",
    "print('Top search results:')\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfc7396",
   "metadata": {},
   "source": [
    "## Test your search engine by asking some questions. \n",
    "\n",
    "Check out the attached questions.json for a few suggested questions to start with, but feel free to play around and search for anything you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7fb83fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3f475cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the questions\n",
    "with open('data/questions.json', 'r') as file:\n",
    "    questions = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5e0577ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Which diseases can be transmitted by animals?',\n",
      "  [{'answer': ' infectious diseases',\n",
      "    'end': 163,\n",
      "    'score': 0.02271362394094467,\n",
      "    'start': 143},\n",
      "   {'answer': ' zoonotic swine flu',\n",
      "    'end': 1051,\n",
      "    'score': 0.0001700880820862949,\n",
      "    'start': 1032},\n",
      "   {'answer': ' pandemic',\n",
      "    'end': 199,\n",
      "    'score': 1.0785701306303963e-05,\n",
      "    'start': 190},\n",
      "   {'answer': ' unprotected sex (including anal and oral sex),',\n",
      "    'end': 877,\n",
      "    'score': 5.843516532877402e-07,\n",
      "    'start': 830},\n",
      "   {'answer': ' biological epidemic outbreaks,',\n",
      "    'end': 286,\n",
      "    'score': 3.396625345430948e-07,\n",
      "    'start': 255}]),\n",
      " ('How many leprosy outbreaks are known to happen?',\n",
      "  [{'answer': ' Seven',\n",
      "    'end': 2554,\n",
      "    'score': 0.12997889518737793,\n",
      "    'start': 2548},\n",
      "   {'answer': ' 31,156',\n",
      "    'end': 1851,\n",
      "    'score': 1.3111383850628044e-05,\n",
      "    'start': 1844},\n",
      "   {'answer': ' about 37.9 million',\n",
      "    'end': 1689,\n",
      "    'score': 6.183402547321748e-06,\n",
      "    'start': 1670},\n",
      "   {'answer': ' 262.', 'end': 94, 'score': 3.9904594473227917e-07, 'start': 89},\n",
      "   {'answer': ' to prevent outbreaks and epidemics from becoming pandemics.',\n",
      "    'end': 236,\n",
      "    'score': 1.213311229264491e-09,\n",
      "    'start': 176}])]\n"
     ]
    }
   ],
   "source": [
    "pprint([(query, find_answer(query)) for query in random.sample(questions, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca773391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
